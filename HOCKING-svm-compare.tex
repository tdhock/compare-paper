\documentclass{article}
\usepackage{listings}
\usepackage{clrscode}
\usepackage[table]{xcolor}
\usepackage{array}
\usepackage{slashbox}
\usepackage{multirow}
\usepackage{fullpage}
\usepackage{tikz}
\usepackage{stmaryrd}
\usepackage{graphicx}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{stfloats}
\usepackage{float}

\renewcommand{\thefootnote}{\fnsymbol{footnote}}

\newtheorem{proposition}{Proposition}

\newtheorem{definition}{Definition}
\newtheorem{theo}{Theorem}    % numérotés par section
\newtheorem{lemma}{Lemma}

\newcommand{\RR}{\mathbb R}
\newcommand{\NN}{\mathbb N}
\newcommand{\pkg}[1]{\texttt{#1}}
\newcommand{\plausibleK}{\textit{plausibleK}}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\maximize}{maximize}
\DeclareMathOperator*{\minimize}{minimize}


\newfloat{Algorithm}{thp}{lop}
\floatname{Algorithm}{Algorithm}

% For citations
\usepackage{natbib}

% For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{hyperref}
\newcommand{\theHalgorithm}{\arabic{algorithm}}

\usepackage{icml2013} 

\icmltitlerunning{Support vector comparison machines}

\begin{document}

\renewcommand{\arraystretch}{1.5}

\definecolor{lightgray}{rgb}{0.9,0.9,0.9}
\definecolor{pastelblue}{RGB}{213,229,255}
\newcolumntype{a}{>{\columncolor{lightgray}}c}

\twocolumn[
\icmltitle{Support vector comparison machines}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2012
% package.


%\icmlauthor{Guillem Rigaill\footnotemark[1]}{rigaill@evry.inra.fr}
%\icmladdress{Unit\'e de Recherche en G\'enomique V\'eg\'etale INRA-CNRS-Universit\'e d'Evry Val d'Essonne, Evry, France}
%\icmlauthor{Toby Dylan Hocking\footnotemark[1], 
% Francis Bach}{toby.hocking@inria.fr, francis.bach@inria.fr}
\icmlauthor{Toby Dylan Hocking}{toby@sg.cs.titech.ac.jp}
\icmlauthor{Supaporn Spanurattana}{supaporn@sg.cs.titech.ac.jp}
\icmlauthor{Masashi Sugiyama}{sugi@cs.titech.ac.jp}
\icmladdress{Department of Computer Science, Tokyo Institute of
  Technology, Tokyo 152-8552, Japan}

% You may provide any keywords that you 
% find helpful for describing your paper; these are used to populate 
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{support vector machine, ranking, comparing, convex,
  optimization, relaxation, libsvm}

\vskip 0.3in
]

\begin{abstract}
  In ranking problems, the goal is to learn a ranking function
  $r(x)\in\RR$ from labeled pairs $x,x'$ of input points. In this
  paper, we consider the related comparison problem, where the label
  $y\in\{-1,0,1\}$ indicates which element of the pair is better, or
  if there is no significant difference. We cast the learning problem
  as a margin maximization, and show that it can be solved by
  converting it to a standard SVM. We compare our algorithm to SVMrank
  using a benchmark ranking data set.
\end{abstract}

\section{Introduction}

In this paper we consider the supervised comparison problem. Assume
that we have $n$ labeled training pairs and for each pair
$i\in\{1,\dots,n\}$ we have input features $x_i,x_i'\in\RR^p$ and a
label $y_i\in\{-1,0,1\}$ that indicates which element is better:
\begin{equation}
  \label{eq:z}
  y_i =
  \begin{cases}
    -1 & \text{ if $x_i$ is better than $x'_i$},\\
    0 & \text{ if $x_i$ is as good as $x'_i$},\\
    1 & \text{ if $x'_i$ is better than $x_i$}.
  \end{cases}
\end{equation}
These data are geometrically represented by segments and arrows in the
top panel of Figure~\ref{fig:geometry}. Comparison data naturally
arise when considering subjective human evaluations of pairs of
items. For example, movies, graph layouts, and web
search results. TODO.

The goal of learning is to find a comparison function $c:\RR^p \times
\RR^p \rightarrow \{-1,0,1\}$ which generalizes to a test set of data:
\begin{equation}
  \minimize_{c} 
  \sum_{i=1}^n
  e\left[ c(x_i, x_i'), y_i \right],
\end{equation}
where $e(\hat y, y) = 1_{\hat y \neq y}$ is the zero-one loss.

This is similar to a ranking problem, for which a number of machine
learning algorithms exist \citep{learning-to-rank}, such as RankSVM
\citep{ranksvm}. There are two key differences between ranking and
comparing, which is what we want to do:
\begin{itemize}
\item Ranking algorithms ignore the $y_i=0$ equality pairs.
\item The goal of ranking is to give an absolute order to a set of
  items, typically documents in a search engine. Comparison is simpler
  since we only need to make a decision about exactly two items.
\end{itemize}

The rest of this article is organized as follows. In
Section~\ref{sec:related} we discuss links with related work on
classification and ranking, then in Section~\ref{sec:svm-compare} we
propose a new algorithm: \proc{SVMcompare}. We show results on a benchmark
dataset in Section~\ref{sec:results} and discuss future work in
Section~\ref{sec:conclusions}.

\section{Related work}
\label{sec:related}

First we discuss connections with several existing methods in the
machine learning literature, and then we discuss how ranking
algorithms can be applied to the comparison problem.

When the inputs are discrete $x_i,x_i'\in\{1,\dots,k\}$, then the
problem is known as learning a relation \citep{relations}. In this
article we only consider the case when inputs are continuous
$x_i,x_i'\in\RR^p$.

\begin{table}[b!]
  \centering
  \begin{tabular}{|a|c|c|}\hline
    \rowcolor{lightgray}
    \backslashbox{Outputs}{Inputs}
    &single items $x$&pairs $x,x'$\\ \hline
    $y\in\{-1,1\}$ &SVM  & SVMrank   	\\ \hline 
    $y\in\{-1,0,1\}$ &Reject option& this work\\ \hline
  \end{tabular}
  \caption{\label{tab:related} Comparison is similar to ranking 
    and classification with reject option.}
\end{table}

\subsection{Rank, reject, and rate}

Some related work appears in Table~\ref{tab:related}:

\begin{itemize}
\item \citet{reject-option} studied the statistical properties of the
  hinge loss for the classification with reject option.
\item \citet{ranksvm} proposed SVM for ranking.
\item \citet{rank-with-ties} proposed a boosting algorithm for ranking
  with ties, and observed that ties are more effective when there are
  more output values.
\item \citet{trueskill} proposed TrueSkill: a Bayesian skill rating
  system, a generalization of the Elo chess rating system.
\end{itemize}

\begin{figure}
  \centering
  \input{figure-norm-data}
  \vskip -0.5cm
  \caption{Geometric interpretation of the comparison
    problem. \textbf{Top}: input feature pairs $x_i,x_i'\in\RR^p$ are
    drawn as segments and arrows, colored using the labels
    $y_i\in\{-1,0,1\}$. The level curves of the underlying ranking
    function $r(x)=||x||_2^2$ are drawn in grey, and differences
    $|r(x)-r(x')|\leq 1$ are considered insignificant
    ($y_i=0$). \textbf{Middle}: in the enlarged feature space, the
    ranking function is linear: $r(x)=w^\intercal
    \Phi(x)$. \textbf{Bottom}: two symmetric hyperplanes
    $w^\intercal[\Phi(x_i')-\Phi(x_i)]\in\{-1,1\}$ are used as a
    comparison function to separate the difference vectors.}
  \label{fig:geometry}
\end{figure}

\subsection{SVMrank for comparing}

In this section we explain how to apply the existing SVMrank algorithm
of \citet{ranksvm} to a comparison data set.

The goal of the SVMrank algorithm is to learn a ranking function
$r:\RR^p \rightarrow \RR$. When $r(x)=w^\intercal x$ is linear, the
primal problem for some cost parameter $C\in\RR^+$ is the following
quadratic program (QP):
\begin{equation}
  \begin{aligned}
    \minimize_{w, \xi}\ \  & \frac 1 2 w^\intercal w 
    + C \sum_{i\in I_1\cup I_{-1}} \xi_i \\
    \text{subject to}\ \  & 
    \forall i\in I_1\cup I_{-1},\ \xi_i \geq 0,\\
&    \text{and }\xi_i \geq 1-w^\intercal(x_i'-x_i)y_i,
  \end{aligned}
  \label{eq:svmrank}
\end{equation}
where $I_y=\{i\mid y_i=y\}$ are the sets of indices for the different
labels. Note that the equality pairs $i$ such that $y_i=0$ are not
used in the optimization problem.

After obtaining an optimal $w\in\RR^p$ by solving (\ref{eq:svmrank}),
we define a comparison function $c_t:\RR^p\times \RR^p\rightarrow
\{-1, 0, 1\}$ for any threshold $t\in\RR^+$:
\begin{equation}
  \label{eq:svmrank_c_t}
  c_t(x, x') =
  \begin{cases}
    -1 & \text{ if } w^\intercal(x' - x) < -t, \\
    0 & \text{ if } |w^\intercal(x' - x)| \leq t, \\
    1 & \text{ if } w^\intercal(x' - x) > t. \\
  \end{cases}
\end{equation}
We can then use grid search to estimate an optimal threshold $\hat t$,
by minimizing the zero-one loss with respect to all the training
pairs:
\begin{equation}
  \hat t = \argmin_{t}
  \sum_{i=1}^n
  e\left[ c_t(x_i, x_i'), y_i \right].
\end{equation}
However, there are two potential problems with the learned comparison
function $c_{\hat t}$. First, the equality pairs $i\in I_0$ are not
used to learn the weight vector $w$ in (\ref{eq:svmrank}). Second, the
threshold $\hat t$ is learned in a separate optimization step, which
may be suboptimal. In the next section, we propose a new algorithm
that fixes these potential problems.

\section{Support vector comparison machines}
\label{sec:svm-compare}

In this section we propose \proc{SVMcompare}, a new learning algorithm
designed for comparison problems.

\subsection{LP and QP for separable data}
\label{sec:lp-qp}

To illustrate the nature of the max-margin comparison problem, in this
section we assume that the training data are linearly separable. Later
in Section~\ref{sec:kernelized-qp}, we propose an algorithm for
learning a nonlinear function from non-separable data.

In the following linear program (LP), we consider learning a linear
ranking function $r(x)=w^\intercal x$ that maximizes the margin $\mu$:
\begin{equation}
  \label{eq:max-margin-lp}
  \begin{aligned}
    \maximize_{\mu\in\RR, w\in\RR^p}\ & \mu \\
    \text{subject to}\ & \mu \leq 1-|w^\intercal (x_i' - x_i)|,\
    \forall\  i\in I_0,\\
    &\mu \leq -1 +  w^\intercal(x_i'-x_i)y_i,\ \forall\ i\in I_1\cup I_{-1}.
  \end{aligned}
\end{equation}
Note that solving this problem is a practical test for linear
separability. If the optimal $\mu>0$ then the data are linearly
separable. The geometric interpretation of the margin is shown in
Figure~\ref{fig:hard-margin}. It is the distance from any difference
vector $x_i'-x_i$ to its nearest decision boundary $r(x)\in\{-1,1\}$.

\begin{figure*}
  \centering
  \input{figure-hard-margin}
  \vskip -1cm
  \caption{The separable LP and QP comparison problems. \textbf{Left}:
    the difference vectors $x'-x$ of the original data and the optimal
    solution to the LP (\ref{eq:max-margin-lp}). \textbf{Middle}: for
    the unscaled flipped data $\tilde x'-\tilde x$ (\ref{eq:tilde}),
    the LP is not the same as the QP
    (\ref{eq:max-margin-qp}). \textbf{Right}: for the scaled flipped
    data, the QP is equivalent to the LP.}
  \label{fig:hard-margin}
\end{figure*}

Another way to formulate the comparison problem is by first performing
a change of variables, and then learning a binary SVM. The idea is to
maximize the margin between significant differences $y_i\in\{-1,1\}$
and equality pairs $y_i=0$. Let $X_y,X_y'$ denote the $|I_y|\times p$
matrices formed by all the pairs $i\in I_y$. We define a new data set
with $m=|I_1|+|I_{-1}|+2|I_0|$ pairs suitable for training a binary
SVM:
\begin{equation}
\label{eq:tilde}
  \tilde X = \left[
    \begin{array}{c}
      X_1 \\
      X_{-1}'\\
      X_0\\
      X_0'
    \end{array}
  \right],\ 
  \tilde X' = \left[
    \begin{array}{c}
      X_1' \\
      X_{-1}\\
      X_0'\\
      X_0
    \end{array}
  \right],\ 
  \tilde y = \left[
    \begin{array}{c}
      1_{|I_1|} \\
      1_{|I_{-1}|}\\
      -1_{|I_0|}\\
      -1_{|I_0|}
    \end{array}
  \right],
\end{equation}
where $1_n$ is an $n$-vector of ones, $\tilde X,\tilde
X'\in\RR^{m\times p}$ and $\tilde y\in\{-1,1\}^m$. Note that $\tilde
y_i=-1$ implies no significant difference between $\tilde x_i$ and
$\tilde x_i'$, and $\tilde y_i=1$ implies that $\tilde x_i$ is better
than $\tilde x_i'$. We then learn an affine function
$f(x)=\beta+u^\intercal x$ using binary SVM:
\begin{equation}
  \label{eq:max-margin-qp-tilde}
  \begin{aligned}
    \minimize_{u\in\RR^p, \beta\in\RR}\ & u^\intercal u  \\
    \text{subject to}\ & 
    \tilde y_i (\beta + u^\intercal( \tilde x_i'-\tilde x_i) ) \geq 1,
    \ \forall i\in\{1,\dots,m\}.
  \end{aligned}
\end{equation}
In the next Lemma, we use this QP
% (\ref{eq:max-margin-qp-tilde}) 
to
construct a ranking function $\hat r(x)= \hat w^\intercal x$ that is
feasible for the original LP (\ref{eq:max-margin-lp}).
\begin{lemma}
  Let $u\in\RR^p,\beta\in\RR$ be a solution of
  (\ref{eq:max-margin-qp-tilde}). Then $\hat \mu = -1/\beta$
  and $\hat w = -u/\beta$ are feasible for
  (\ref{eq:max-margin-lp}).
  \label{lemma:feasible}
\end{lemma}
\begin{proof}
  Begin by assuming that we want to find a ranking function $r(x)=\hat
  w^\intercal x = \gamma u^\intercal x$, where $\gamma\in\RR$ is a
  scaling constant.  Then consider that for all $x$ on the decision
  boundary, we have the following two equalities
  \begin{equation}
    \label{eq:dec-boundary-rank}
    u
  \end{equation}
and TODO
\begin{equation}
  \label{eq:max-margin-qp}
  \begin{aligned}
    \minimize_{u\in\RR^p, \beta\in\RR}\ & u^\intercal u  \\
    \text{subject to}\ & \beta + |u^\intercal (x_i'-x_i)| \leq -1,\
    \forall\  i\in I_0,\\
    & \beta + u^\intercal(x_i'-x_i)y_i \geq 1,\ \forall\ i\in I_1\cup I_{-1}.
  \end{aligned}
\end{equation}
\end{proof}


\subsection{Kernelized QP for non-separable data}
\label{sec:kernelized-qp}
In this section, we assume the data are not separable, and want to
learn a general nonlinear ranking function. We define a positive
definite kernel $\kappa:\RR^p\times \RR^p\rightarrow\RR$, which
implicitly defines features $\Phi(x)$. As in
(\ref{eq:max-margin-qp-tilde}), we learn a function $f(x)=\beta +
u^\intercal \Phi(x)$ which is affine in the feature space. Let
$\alpha,\alpha'\in\RR^m$ be coefficients such that $u=\sum_{i=1}^m
\alpha_i \Phi(\tilde x_i) + \alpha_i' \Phi(\tilde x_i')$, and so we
have
%the learned function is thus
 $f(x) =\beta + \sum_{i=1}^m \alpha_i \kappa(\tilde x_i, x) +
\alpha_i' \kappa(\tilde x_i', x)$. We then use Lemma~\ref{lemma:feasible} to
define the ranking function
\begin{equation}
  \label{eq:kernelized_r}
  r(x)= \frac{u^\intercal \Phi(x)}{-\beta} = \frac{1}{-\beta}
  \sum_{i=1}^m \left[
    \alpha_i \kappa(\tilde x_i, x) + \alpha_i'  \kappa(\tilde x_i', x) 
  \right],
\end{equation}
which implies the comparison function
\begin{equation}
  \label{eq:kernelized_c}
  c(x, x') =
  \begin{cases}
    -1 & \text{if } r(x') - r(x) < -1, \\
    0 & \text{if } |r(x') - r(x)| \leq 1, \\
    1 & \text{if } r(x') - r(x) > 1. \\
  \end{cases}
\end{equation}

Let $K=[K_1\cdots K_m\ K_1'\cdots K_m']\in\RR^{2m\times 2m}$ be the
kernel matrix, where for all pairs $i\in\{1, \dots, m\}$, the kernel
vectors $K_i,K_i'\in\RR^{2m}$ are defined as
\begin{equation}
  K_i = \left[
    \begin{array}{c}
      \kappa(\tilde x_1, \tilde x_i)\\
      \vdots\\
      \kappa(\tilde x_m, \tilde x_i)\\
      \kappa(\tilde x_1', \tilde x_i)\\
      \vdots\\
      \kappa(\tilde x_m', \tilde x_i)
    \end{array}
  \right],\ 
  K_i' = \left[
    \begin{array}{c}
      \kappa(\tilde x_1, \tilde x_i')\\
      \vdots\\
      \kappa(\tilde x_m, \tilde x_i')\\
      \kappa(\tilde x_1', \tilde x_i')\\
      \vdots\\
      \kappa(\tilde x_m', \tilde x_i')
    \end{array}
  \right].
\end{equation}
Letting $a=[\alpha^\intercal\
\alpha'^\intercal]^\intercal\in\RR^{2m}$, The norm of the linear
function in the feature space is $w^\intercal w = a^\intercal K a$,
and we can write the primal soft-margin SVM problem for some
$C\in\RR^+$ as
\begin{equation}
  \begin{aligned}
      \minimize_{a\in\RR^{2m},\xi\in\RR^m,\beta\in\RR}\ \ & 
      \frac 1 2 a^\intercal K a + C\sum_{i=1}^m \xi_i \\
      \text{subject to}\ \ & 
      \text{for all $i\in\{1,\dots,m\}$, }
      \xi_i \geq 0,\\
      &\text{and }
      \xi_i \geq 1-\tilde y_i(\beta + a^\intercal (K_i'-K_i)).
  \end{aligned}
\end{equation}
TODO: write the Lagrangian and dual feasibility conditions.

If $v\in\RR^m$ are the dual variables corresponding to the second
constraint, then the dual problem is
\begin{equation}
  \begin{aligned}
    \label{eq:svm-dual}
    \minimize_{v\in\RR^m}\ \ &
    \frac 1 2 v^\intercal Y M^\intercal K M Y v - v^\intercal 1\\
    \text{subject to}\ \ &
    \sum_{i=1}^n v_i y_i = 0,\\
&    \text{for all $i\in\{1,\dots,n\}$, } 0\leq v_i\leq C,
  \end{aligned}
\end{equation}
where $M=[-I_n \, I_n]^\intercal\in\{-1,0,1\}^{2n\times n}$. This quadratic problem
is equivalent to a standard SVM with kernel $M^\intercal K M$.

So we can use any efficient SVM solver, such as libsvm
\citep{libsvm}. We used the R interface in the \texttt{kernlab}
package of \citet{kernlab}, and our code is available in the
\texttt{rankSVMcompare} package on Github.

After solving the SVM dual problem (\ref{eq:svm-dual}), we recover the
primal variables $a = MYv$ and the bias $\beta$ using the usual method
(TODO). The learned ranking function is $r(x) = \sum_{i\in\text{sv}}
-a_i \kappa(x, x_i)/\beta$ (TODO...Z??).

\proc{SVMcompare} is summarized as Algorithm~\ref{alg:SVMcompare}.

\begin{algorithm}[b!]
   \caption{\proc{SVMcompare}}
   \label{alg:SVMcompare}
\begin{algorithmic}
  \STATE {\bfseries Input:} cost $C\in\RR^+$, kernel
  $\kappa:\RR^p\times \RR^p \rightarrow \RR$, features $X,X'\in\RR^{n \times p}$,
  labels $y\in\{-1,0,1\}^n$.

  \STATE \makebox[0.5cm]{$\tilde X$} $\gets [$
  \makebox[1cm]{$X_1^\intercal$}
  \makebox[1cm]{$X_{-1}'^\intercal$}
  \makebox[1cm]{$X_0^\intercal$}
  \makebox[1cm]{$X_0'^\intercal$}
  $]^\intercal$.

  \STATE \makebox[0.5cm]{$\tilde X'$} $\gets [$
  \makebox[1cm]{$X_1'^\intercal$}
  \makebox[1cm]{$X_{-1}^\intercal$}
  \makebox[1cm]{$X_0'^\intercal$}
  \makebox[1cm]{$X_0^\intercal$}
  $]^\intercal$.

  \STATE \makebox[0.5cm]{$\tilde y$} $\gets [$
  \makebox[1cm]{$1_{|I_1|}^\intercal$}
  \makebox[1cm]{$1_{|I_{-1}|}^\intercal$}
  \makebox[1cm]{$-1_{|I_0|}^\intercal$}
  \makebox[1cm]{$-1_{|I_0|}^\intercal$}
  $]^\intercal$.

  \STATE $K \gets \proc{KernelMatrix}(\tilde X, \tilde X', \kappa)$.

  \STATE $M \gets [ -I_m\ I_m ]^\intercal$.

  \STATE $\tilde K \gets M^\intercal K M$.

  \STATE $v,\beta \gets \proc{SVMdual}(\tilde K, \tilde y, C)$.

  \STATE $\alpha_i \gets $ 
  \makebox[1cm][r]{$-\tilde y_i v_i$},
  $\forall i\in\{1,\dots, m\}$.

  \STATE $\alpha_i' \gets $
  \makebox[1cm][r]{$\tilde y_i v_i$},
  $\forall i\in\{1,\dots, m\}$.

  \STATE {\bfseries Output:} coefficients $\alpha,\alpha'\in\RR^{m}$,
  bias $\beta\in\RR$, ranking $r$ (\ref{eq:kernelized_r}) and
  comparison $c$ (\ref{eq:kernelized_c}) functions.

   \end{algorithmic}
\end{algorithm}

\section{Results}
\label{sec:results}

For evaluation, we propose the zero-one loss function
$e:\{-1,0,1\}\times\{-1,0,1\}\rightarrow\{0,1\}$, described in
Table~\ref{tab:evaluation}.

\begin{table}[b!]
  \centering
  \begin{tabular}{|a|c|c|c|}\hline
    \rowcolor{lightgray}
    \backslashbox{$\hat{y}$}{ $y$}
    &\textbf{-1}&\textbf{0}&\textbf{1}\\ \hline
    \textbf{-1}&0  & FP & Inversion   	\\ \hline 
    \textbf{0} &FN& 0& FN\\ \hline
    \textbf{1} & Inversion & FP &0	\\ \hline
  \end{tabular}
  % \cellcolor{pastelblue}
  \caption{We use the zero-one loss to evaluate a predicted label
    $\hat y$ given the true label $y$. False positives (FP) occur 
    when predicting a significant difference $\hat y\in\{-1,1\}$ 
    when there is none $y=0$, and False Negatives (FN) are the opposite.
  Inversions occur when predicting the opposite of the true label
  $\hat y = -y$.}
  \label{tab:evaluation}
\end{table}

\section{Conclusions and future work}
\label{sec:conclusions}

\bibliographystyle{abbrvnat}
\bibliography{refs}

\end{document}

